{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Secrets & API-key bootstrap"
      ],
      "metadata": {
        "id": "Rnt1zYKUtYy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────\n",
        "#  SECRETS → LOCAL FILES  |  runs before any other import\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "import os, base64, pathlib, json\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "def _decode_and_validate(secret_name: str, out_file: str):\n",
        "    \"\"\"Decode Base64 GitHub secret and validate JSON structure.\"\"\"\n",
        "    try:\n",
        "        decoded = base64.b64decode(os.environ[secret_name]).decode('utf-8')\n",
        "        data = json.loads(decoded)\n",
        "\n",
        "        # For token.json, verify required fields\n",
        "        if out_file == \"token.json\":\n",
        "            required_fields = {'token', 'refresh_token', 'scopes'}\n",
        "            if not required_fields.issubset(data.keys()):\n",
        "                missing = required_fields - set(data.keys())\n",
        "                raise ValueError(f\"Missing required fields in token: {missing}\")\n",
        "\n",
        "        pathlib.Path(out_file).write_text(decoded)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {secret_name}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    _decode_and_validate(\"CLIENT_SECRETS_JSON\", \"client_secrets.json\")\n",
        "    _decode_and_validate(\"TOKEN_JSON\", \"token.json\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Failed to initialize secrets\")\n",
        "    raise\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]  # <-- add this line\n",
        "client = OpenAI()\n",
        "print(\"✅ Secrets decoded and validated, OpenAI client ready.\")\n"
      ],
      "metadata": {
        "id": "mGMYG1pYtZ2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Parameter cell"
      ],
      "metadata": {
        "id": "zMHzrGLkuV5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS  (Papermill / run-notebook overwrites these at run-time)\n",
        "GAMEPLAY_URL = None                         # GitHub secret injects real link\n",
        "TITLE_TEXT   = \"Daily DALLE Short\"          # you can override this too\n",
        "OPENAI_MODEL = \"gpt-4o\"\n"
      ],
      "metadata": {
        "id": "7LF15yK-ucTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔐 3. Authenticate with YouTube"
      ],
      "metadata": {
        "id": "SHzZ3v0lAbeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔐 2. Authenticate with YouTube (automatic – no browser needed)\n",
        "from google.oauth2.credentials import Credentials\n",
        "from google.auth.transport.requests import Request\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import json\n",
        "\n",
        "# Use only the essential scope needed for uploads\n",
        "SCOPE = \"https://www.googleapis.com/auth/youtube.upload\"\n",
        "\n",
        "def get_authenticated_service():\n",
        "    \"\"\"Create authenticated YouTube client with scope validation.\"\"\"\n",
        "    try:\n",
        "        # Load token data\n",
        "        with open(\"token.json\") as f:\n",
        "            token_data = json.load(f)\n",
        "\n",
        "        # Verify the token has our required scope\n",
        "        if 'scopes' not in token_data or SCOPE not in token_data['scopes']:\n",
        "            raise ValueError(f\"Token missing required scope: {SCOPE}\")\n",
        "\n",
        "        creds = Credentials.from_authorized_user_info(token_data, [SCOPE])\n",
        "\n",
        "        # Refresh token if needed\n",
        "        if creds and creds.expired and creds.refresh_token:\n",
        "            try:\n",
        "                creds.refresh(Request())\n",
        "                # Update token file with refreshed credentials\n",
        "                with open(\"token.json\", \"w\") as f:\n",
        "                    json.dump(json.loads(creds.to_json()), f)\n",
        "            except Exception as refresh_error:\n",
        "                print(f\"⚠️ Token refresh failed: {refresh_error}\")\n",
        "                # Continue with expired token if we have one\n",
        "                if not creds.token:\n",
        "                    raise\n",
        "\n",
        "        return build(\"youtube\", \"v3\", credentials=creds)\n",
        "    except HttpError as e:\n",
        "        print(f\"❌ YouTube API error: {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Authentication failed: {e}\")\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    youtube = get_authenticated_service()\n",
        "    print(f\"✅ YouTube API authenticated with scope: {SCOPE}\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Failed to initialize YouTube client\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "MPFDRFqu9iiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-7: THIS IS WHERE WE BUILD OUR SHORTS DRAMA STORIES GENERATION CODE BLOCKS"
      ],
      "metadata": {
        "id": "_hFbm_U9oov0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Generate a Unique Story Seed (Topic)"
      ],
      "metadata": {
        "id": "FjxmsRiHux6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────────\n",
        "# 1️⃣ Generate a unique seed/topic for each short story run from Reddit r/WritingPrompts\n",
        "\n",
        "import random, time, re\n",
        "import praw\n",
        "\n",
        "def generate_story_seed():\n",
        "    # 1) Initialize PRAW Reddit client (use your credentials or environment variables)\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=\"IPF2FtGAO8sLhzTUbEGSsQ\",\n",
        "        client_secret=\"RLO_IjGT9qIjnaYPxlVctFF3XL4Cjg\",\n",
        "        user_agent=\"ShortStoryBot by /u/Rexxus25\"\n",
        "    )\n",
        "\n",
        "    # 2) Fetch top submissions from r/WritingPrompts in the past 48 hours\n",
        "    sub = reddit.subreddit(\"WritingPrompts\")\n",
        "    now = time.time()\n",
        "    two_days_ago = now - 48 * 3600\n",
        "\n",
        "    # Grab the top 50 from the past week, filter to last 48 h, sort by score\n",
        "    posts = list(sub.top(time_filter=\"week\", limit=50))\n",
        "    recent = [p for p in posts if p.created_utc >= two_days_ago and not p.stickied]\n",
        "    recent_sorted = sorted(recent, key=lambda p: p.score, reverse=True)\n",
        "    top20 = recent_sorted[:20] if len(recent_sorted) >= 20 else recent_sorted\n",
        "\n",
        "    # If not enough recent, fallback to whatever we have\n",
        "    if not top20:\n",
        "        top20 = posts[:20]\n",
        "\n",
        "    # 3) Clean up the prompt: remove any [XYZ] at the start, e.g. [WP], [PI], etc.\n",
        "    def clean_title(title):\n",
        "        return re.sub(r\"^\\[[^\\]]+\\]\\s*\", \"\", title).strip()\n",
        "\n",
        "    # 4) Pick one at random and clean it\n",
        "    selected = random.choice(top20)\n",
        "    seed = clean_title(selected.title)\n",
        "\n",
        "    print(f\"🔄 Auto‑selected story seed: {seed}\")\n",
        "    return seed\n",
        "\n",
        "STORY_SEED = generate_story_seed()\n"
      ],
      "metadata": {
        "id": "sVX2sFWeuzLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Generate a Short Story Script Using GPT-4o-mini"
      ],
      "metadata": {
        "id": "XFCg4YXou0hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────────\n",
        "# 2️⃣ Generate a viral short story script (<50 seconds, high retention) with GPT-4o-mini, plus descriptions for consistency\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "def generate_short_story(seed, example_story=None):\n",
        "    system_prompt = (\n",
        "        \"You are a viral short story writer for YouTube. \"\n",
        "        \"Write a story designed for narration in under 40 seconds. \"\n",
        "        \"The story must be no more than 130 words. \"\n",
        "        \"In the very first sentence, clearly establish the setting and main character in just a few words before the action starts, as to answer the audiences question of why am i watching this. \"\n",
        "        \"Start immediately with a dramatic moment, conflict, or shocking discovery. \"\n",
        "        \"Use stronghuman emotional triggers, something like betrayal, revenge, sacrifice, secrets, danger, or loss. \"\n",
        "        \"Keep the pace brisk; every sentence should advance the drama. \"\n",
        "        \"Include at some dialogue, but mainly storytell. \"\n",
        "        \"End with a twist, punchline, or moment of realization. This is important and should give the clear message and lesson learned and leave the audience with some food for thought \"\n",
        "        \"Conclude with a clear final line that delivers the story’s lesson, message, or a thought-provoking statement that makes the audience wonder about the situation or story told. \"\n",
        "        \"Make the audience feel why they watched the story by having this clear conclusion or ending message or lessen learned from the story. \"\n",
        "        \"Make sure the audience understands the who/where/when within the first 2 sentences before plunging into the conflict. \"\n",
        "        \"Be direct and vivid. No introduction or explanation—write only the story. \"\n",
        "        \"Limit the story to 130 words maximum.\"\n",
        "    )\n",
        "    few_shot_example = (\n",
        "        example_story or\n",
        "    \"\"\"\n",
        "At midnight, in the rain-soaked alley behind the city’s last jazz club, Mia clutched her broken saxophone.\n",
        "\n",
        "A stranger’s shadow fell across her, voice cold: “Play, or you’ll never see your brother again.”\n",
        "\n",
        "Mia’s trembling fingers obeyed, music mingling with sirens as the night spiraled into chaos.\n",
        "\n",
        "The masked man leaned close. “You think you’re the only one who lost something tonight?”\n",
        "\n",
        "Only later, as dawn broke and the alley was empty, did Mia realize—sometimes you must lose your song to find your voice.\n",
        "\n",
        "Trust the night, but never trust its promises.\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        f\"{system_prompt}\\n\"\n",
        "        f\"Example structure:\\n{few_shot_example}\\n\\n\"\n",
        "        f\"Now, write a unique and engaging story based on this topic:\\n\\\"{seed}\\\"\"\n",
        "    )\n",
        "\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=510,\n",
        "        temperature=1,\n",
        "    )\n",
        "    story_script = response.choices[0].message.content.strip()\n",
        "    print(f\"Generated short story:\\n{story_script}\")\n",
        "\n",
        "    # Main character appearance (as before)\n",
        "    char_desc_prompt = (\n",
        "        \"Based on the following short story, describe the main character's appearance (age, gender, hair, eyes, clothing, skin color, facial characteristics other notable features) in one vivid sentence. \"\n",
        "        \"Make up plausible details if they are not specified. Do NOT describe their personality, only looks. \"\n",
        "        \"Output only the appearance description, no extra text.\\n\\n\"\n",
        "        f\"Story:\\n{story_script}\\n\"\n",
        "    )\n",
        "    char_resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": char_desc_prompt}],\n",
        "        max_tokens=50,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    main_character_description = char_resp.choices[0].message.content.strip().replace('\"', '')\n",
        "\n",
        "    # Supporting characters\n",
        "    supp_char_prompt = (\n",
        "        \"Based on the following short story, briefly describe up to two other important characters if present (name, age, gender, appearance, clothing, skin color). \"\n",
        "        \"Output a single sentence for each, or write 'None' if no supporting characters are clearly present.\\n\\n\"\n",
        "        f\"Story:\\n{story_script}\\n\"\n",
        "    )\n",
        "    supp_char_resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": supp_char_prompt}],\n",
        "        max_tokens=90,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    supporting_characters_description = supp_char_resp.choices[0].message.content.strip().replace('\"', '')\n",
        "\n",
        "    # Setting/environment\n",
        "    env_prompt = (\n",
        "        \"Based on the following short story, describe the main setting or environment (place, time of day, atmosphere, surroundings and animation style) in one vivid sentence. \"\n",
        "        \"Do NOT mention the main character here. Output only the description, no extra text.\\n\\n\"\n",
        "        f\"Story:\\n{story_script}\\n\"\n",
        "    )\n",
        "    env_resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": env_prompt}],\n",
        "        max_tokens=60,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    setting_description = env_resp.choices[0].message.content.strip().replace('\"', '')\n",
        "\n",
        "    print(f\"Main character description: {main_character_description}\")\n",
        "    print(f\"Supporting characters: {supporting_characters_description}\")\n",
        "    print(f\"Setting/environment: {setting_description}\")\n",
        "\n",
        "    return story_script, main_character_description, supporting_characters_description, setting_description\n",
        "\n",
        "STORY_SCRIPT, MAIN_CHARACTER_DESC, SUPP_CHAR_DESC, SETTING_DESC = generate_short_story(STORY_SEED)\n"
      ],
      "metadata": {
        "id": "e2dL_Pbcu1rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3️⃣ Generate Relevant Images with DALL·E 3 (Segmented for Story)"
      ],
      "metadata": {
        "id": "g_MA9URBu3-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────────\n",
        "# 3️⃣ Generate relevant images for story segments using DALL·E 3, ensuring visual consistency\n",
        "\n",
        "import math\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "import re\n",
        "\n",
        "def pad_to_vertical(img_path, target_size=(1080, 1920), fill_color=(0, 0, 0)):\n",
        "    img = Image.open(img_path)\n",
        "    img = ImageOps.contain(img, target_size, method=Image.LANCZOS)\n",
        "    pad_img = Image.new(\"RGB\", target_size, fill_color)\n",
        "    offset_x = (target_size[0] - img.width) // 2\n",
        "    offset_y = (target_size[1] - img.height) // 2\n",
        "    pad_img.paste(img, (offset_x, offset_y))\n",
        "    pad_img.save(img_path)\n",
        "    return img_path\n",
        "\n",
        "def split_script_for_images(script, num_images=7):\n",
        "    sentences = re.split(r'(?<=[.!?]) +', script)\n",
        "    avg = math.ceil(len(sentences) / num_images)\n",
        "    segments = [' '.join(sentences[i:i + avg]) for i in range(0, len(sentences), avg)]\n",
        "    return segments[:num_images]\n",
        "\n",
        "def get_visual_prompt(client, segment, main_character_description, supporting_characters_description, setting_description):\n",
        "    prompt = (\n",
        "        \"You are creating a DALL·E 3 prompt for a cinematic illustration. \"\n",
        "        \"For the following story segment, pick the most visually engaging scene and choose the best shot type (wide, medium, or close-up) to convey the drama. \"\n",
        "        f\"The main character: {main_character_description}. \"\n",
        "        f\"Supporting characters: {supporting_characters_description}. \"\n",
        "        f\"Setting/environment: {setting_description}. \"\n",
        "        \"The image should depict the main character and (if present) supporting character(s) interacting within the environment, performing an action, dialogue, or displaying emotion relevant to the story segment. \"\n",
        "        \"Make the composition consistent with the same people and setting throughout the sequence, even if the angle changes. \"\n",
        "        \"Be visually vivid and concise (max 25 words). \"\n",
        "        f\"Story segment: {segment}\"\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert at crafting prompts for cinematic storybook illustrations.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=120,\n",
        "        temperature=0.85,\n",
        "    )\n",
        "    return response.choices[0].message.content.strip().strip('\"')\n",
        "\n",
        "def generate_dalle3_images(story_script, main_character_description, supporting_characters_description, setting_description, num_images=7):\n",
        "    client = OpenAI()\n",
        "    segments = split_script_for_images(story_script, num_images)\n",
        "    image_files = []\n",
        "    for i, segment in enumerate(segments):\n",
        "        visual_prompt = get_visual_prompt(client, segment, main_character_description, supporting_characters_description, setting_description)\n",
        "        dalle_prompt = f\"Cinematic illustration, {visual_prompt}\"\n",
        "        print(f\"🖼️ Prompt for DALL·E image {i + 1}: {dalle_prompt}\")\n",
        "        try:\n",
        "            response = client.images.generate(\n",
        "                model=\"dall-e-3\",\n",
        "                prompt=dalle_prompt,\n",
        "                n=1,\n",
        "                size=\"1024x1024\",\n",
        "                quality=\"standard\",\n",
        "                response_format=\"url\"\n",
        "            )\n",
        "            img_url = response.data[0].url\n",
        "            img_data = requests.get(img_url).content\n",
        "            img_path = f\"story_img_{i + 1}.png\"\n",
        "            with open(img_path, \"wb\") as f:\n",
        "                f.write(img_data)\n",
        "            pad_to_vertical(img_path)\n",
        "            image_files.append(img_path)\n",
        "            print(f\"Generated image for segment {i + 1}: {img_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ DALL·E image generation failed for segment {i + 1}: {e}\")\n",
        "            continue\n",
        "    return image_files, segments\n",
        "\n",
        "IMAGE_FILES, STORY_SEGMENTS = generate_dalle3_images(\n",
        "    STORY_SCRIPT, MAIN_CHARACTER_DESC, SUPP_CHAR_DESC, SETTING_DESC, num_images=7\n",
        ")\n"
      ],
      "metadata": {
        "id": "StzkMgseu4zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4️⃣ Generate Narration with OpenAI TTS (Echo Voice)"
      ],
      "metadata": {
        "id": "C2DdVPtuu6X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────────\n",
        "# 4️⃣ Generate TTS narration using OpenAI tts-1 (random dramatic voice, NEW API)\n",
        "\n",
        "import random\n",
        "from openai import OpenAI\n",
        "\n",
        "def generate_tts_narration(story_script, output_path=\"narration.mp3\"):\n",
        "    client = OpenAI()  # Uses API key from environment\n",
        "\n",
        "    # Choose a random dramatic voice\n",
        "    voices = [\"fable\", \"onyx\", \"echo\"]\n",
        "    selected_voice = random.choice(voices)\n",
        "    print(f\"🎤 Selected TTS voice: {selected_voice}\")\n",
        "\n",
        "    tts_response = client.audio.speech.create(\n",
        "        model=\"tts-1\",\n",
        "        voice=selected_voice,\n",
        "        input=story_script,\n",
        "        response_format=\"mp3\"\n",
        "    )\n",
        "    tts_response.stream_to_file(output_path)\n",
        "    print(f\"Narration audio saved to {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "NARRATION_AUDIO_PATH = generate_tts_narration(STORY_SCRIPT)\n"
      ],
      "metadata": {
        "id": "NbaFuvA0u6xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5️⃣ Generate Synced Subtitles (GPT for Chunks Matching Narration Timing)"
      ],
      "metadata": {
        "id": "zis51G8Ju-Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────────\n",
        "# 5️⃣ Generate subtitle chunks using GPT and sync to narration (NEW API)\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from aeneas.executetask import ExecuteTask\n",
        "from aeneas.task import Task\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def split_text_into_max_lines(text, max_width_chars=25, max_lines=4):\n",
        "    \"\"\"\n",
        "    Split text into multiple chunks where each chunk is max_lines (using word wrapping).\n",
        "    Returns a list of lines chunks (each a string).\n",
        "    \"\"\"\n",
        "    import textwrap\n",
        "    wrapper = textwrap.TextWrapper(\n",
        "        width=max_width_chars,\n",
        "        break_long_words=True,\n",
        "        break_on_hyphens=True\n",
        "    )\n",
        "    # Wrap full text to lines\n",
        "    lines = wrapper.wrap(text)\n",
        "    # Now group lines into max_lines per chunk\n",
        "    chunks = []\n",
        "    for i in range(0, len(lines), max_lines):\n",
        "        chunk = \"\\n\".join(lines[i:i + max_lines])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def chunk_script_for_subtitles(story_script, narration_path, max_width_chars=25, max_lines=4):\n",
        "    \"\"\"\n",
        "    Generate subtitle chunks with precise timing using forced alignment.\n",
        "    Now also guarantees that no subtitle chunk ever exceeds max_lines when word-wrapped.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    import textwrap\n",
        "\n",
        "    # 1. Create transcript file for aeneas\n",
        "    with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.txt') as tf:\n",
        "        tf.write(story_script)\n",
        "        transcript_path = tf.name\n",
        "\n",
        "    # 2. Configure and run aeneas forced alignment\n",
        "    config_string = u\"task_language=eng|is_text_type=plain|os_task_file_format=json\"\n",
        "    task = Task(config_string=config_string)\n",
        "    task.audio_file_path_absolute = narration_path\n",
        "    task.text_file_path_absolute = transcript_path\n",
        "    task.sync_map_file_path_absolute = transcript_path + \".json\"\n",
        "\n",
        "    ExecuteTask(task).execute()\n",
        "    task.output_sync_map_file()\n",
        "\n",
        "    # 3. Parse the sync map and do post-processing split\n",
        "    with open(task.sync_map_file_path_absolute, 'r') as f:\n",
        "        sync_map = json.load(f)\n",
        "\n",
        "    subtitle_timings = []\n",
        "    for fragment in sync_map[\"fragments\"]:\n",
        "        text = fragment[\"lines\"][0].strip()\n",
        "        start = float(fragment[\"begin\"])\n",
        "        end = float(fragment[\"end\"])\n",
        "        duration = end - start\n",
        "\n",
        "        # Split long lines to 4-lines-or-less chunks\n",
        "        chunks = split_text_into_max_lines(text, max_width_chars=max_width_chars, max_lines=max_lines)\n",
        "        chunks = [chunk for chunk in chunks if chunk.strip()]  # filter out empty/whitespace\n",
        "        if not chunks:\n",
        "            continue  # skip fragments with no text\n",
        "        elif len(chunks) == 1:\n",
        "            subtitle_timings.append({\"text\": chunks[0], \"start\": start, \"end\": end})\n",
        "        else:\n",
        "            chunk_duration = duration / len(chunks)\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_start = start + i * chunk_duration\n",
        "                chunk_end = chunk_start + chunk_duration\n",
        "                subtitle_timings.append({\"text\": chunk, \"start\": chunk_start, \"end\": chunk_end})\n",
        "\n",
        "    # 4. Clean up temp files\n",
        "    os.remove(transcript_path)\n",
        "    os.remove(task.sync_map_file_path_absolute)\n",
        "\n",
        "    return subtitle_timings\n",
        "\n",
        "\n",
        "# Install mutagen for MP3 duration reading if needed\n",
        "!pip install mutagen --quiet\n",
        "\n",
        "SUBTITLES = chunk_script_for_subtitles(STORY_SCRIPT, NARRATION_AUDIO_PATH)\n"
      ],
      "metadata": {
        "id": "ddfDpm_8u-5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6️⃣ Combine All Into Shorts Video (Images + Narration + Subtitles)"
      ],
      "metadata": {
        "id": "V9qTqlW5vAiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────────\n",
        "import sys\n",
        "\n",
        "if sys.platform.startswith(\"linux\"):\n",
        "    TEXT_FONT_PATH = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
        "elif sys.platform.startswith(\"win\"):\n",
        "    TEXT_FONT_PATH = \"C:/Windows/Fonts/DejaVuSans-Bold.ttf\"\n",
        "else:\n",
        "    TEXT_FONT_PATH = \"DejaVuSans-Bold.ttf\"\n",
        "TEXT_FONTSIZE = 52\n",
        "\n",
        "def _set_duration(clip, duration):\n",
        "    if hasattr(clip, \"with_duration\"): return clip.with_duration(duration)\n",
        "    return clip.set_duration(duration)\n",
        "\n",
        "def _set_audio(clip, audio):\n",
        "    if hasattr(clip, \"with_audio\"): return clip.with_audio(audio)\n",
        "    return clip.set_audio(audio)\n",
        "\n",
        "def _set_position(clip, pos):\n",
        "    if hasattr(clip, \"with_position\"): return clip.with_position(pos)\n",
        "    return clip.set_position(pos)\n",
        "\n",
        "def _set_start(clip, start):\n",
        "    if hasattr(clip, \"with_start\"): return clip.with_start(start)\n",
        "    return clip.set_start(start)\n",
        "\n",
        "def resize_fx(clip, newsize):\n",
        "    if hasattr(clip, \"resized\"):\n",
        "        return clip.resized(newsize)\n",
        "    return clip.resize(newsize)\n",
        "\n",
        "from moviepy import ImageClip, TextClip, CompositeVideoClip, concatenate_videoclips, AudioFileClip\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_for_width(text, max_width_chars=25, max_lines=4):\n",
        "    \"\"\"Wrap text to fit within video frame width and limit to max_lines.\"\"\"\n",
        "    wrapper = textwrap.TextWrapper(\n",
        "        width=max_width_chars,\n",
        "        break_long_words=True,\n",
        "        break_on_hyphens=True,\n",
        "        max_lines=max_lines,\n",
        "        placeholder=' [...]'\n",
        "    )\n",
        "    return \"\\n\".join(wrapper.wrap(text))\n",
        "\n",
        "def robust_textclip(text, start, end, font_path, fontsize):\n",
        "    \"\"\"\n",
        "    Create a TextClip for subtitles, ensuring consistent formatting.\n",
        "    \"\"\"\n",
        "    duration = end - start\n",
        "    txt = None\n",
        "\n",
        "    # Wrap text to ensure it fits within frame and limits to max lines\n",
        "    wrapped_text = wrap_text_for_width(text, max_width_chars=25, max_lines=4)\n",
        "\n",
        "    # Try caption mode with proper size constraints\n",
        "    try:\n",
        "        txt = TextClip(\n",
        "            wrapped_text,\n",
        "            font_size=fontsize,\n",
        "            color='white',\n",
        "            method='caption',\n",
        "            size=(900, None)  # Max width for 1080px frame with margins\n",
        "        )\n",
        "        return txt\n",
        "    except Exception as e:\n",
        "        print(f\"[Subtitle/caption failover] {e}\")\n",
        "\n",
        "    # Try label mode with font, fontsize\n",
        "    try:\n",
        "        txt = TextClip(\n",
        "            wrapped_text,\n",
        "            fontsize=fontsize,\n",
        "            color='white',\n",
        "            font=font_path,\n",
        "            method='label'\n",
        "        )\n",
        "        return txt\n",
        "    except Exception as e:\n",
        "        print(f\"[Subtitle/label failover] {e}\")\n",
        "\n",
        "    # Try plain text, no font specified\n",
        "    try:\n",
        "        txt = TextClip(wrapped_text)\n",
        "        return txt\n",
        "    except Exception as e:\n",
        "        print(f\"[Subtitle/plain failover] {e}\")\n",
        "\n",
        "    # Last resort: Render text with Pillow, use ImageClip\n",
        "    try:\n",
        "        from PIL import Image, ImageDraw, ImageFont\n",
        "        import numpy as np\n",
        "\n",
        "        W, H = 900, 200  # Subtitle box (width, height)\n",
        "        bg_color = (0, 0, 0, 90)  # Semi-transparent background\n",
        "        fg_color = (255, 255, 255, 255)\n",
        "\n",
        "        # Load font, fallback to default PIL font if error\n",
        "        try:\n",
        "            font = ImageFont.truetype(font_path, fontsize)\n",
        "        except Exception as e:\n",
        "            print(f\"[Subtitle/Pillow] Can't load '{font_path}', using default font: {e}\")\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "        # Use the pre-wrapped text\n",
        "        final_text = wrapped_text\n",
        "\n",
        "        # Dummy image for measuring\n",
        "        dummy_img = Image.new(\"RGBA\", (W, H))\n",
        "        draw = ImageDraw.Draw(dummy_img)\n",
        "        bbox = draw.textbbox((0, 0), final_text, font=font)\n",
        "        text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
        "\n",
        "        # Ensure text fits within our box width\n",
        "        if text_width > W - 40:  # 40px total margin\n",
        "            # If still too wide, try smaller font or more aggressive wrapping\n",
        "            scale_factor = (W - 40) / text_width\n",
        "            adjusted_fontsize = int(fontsize * scale_factor)\n",
        "            try:\n",
        "                font = ImageFont.truetype(font_path, adjusted_fontsize)\n",
        "            except:\n",
        "                font = ImageFont.load_default()\n",
        "\n",
        "        # Now render actual image with proper dimensions\n",
        "        margin_px = 60\n",
        "        img_height = max(H, text_height + 40 + margin_px)\n",
        "        img = Image.new('RGBA', (W, img_height), bg_color)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "\n",
        "        # Center the text horizontally and vertically within the box\n",
        "        text_x = (W - text_width) // 2\n",
        "        text_y = (img_height - text_height - margin_px) // 2\n",
        "\n",
        "        draw.text(\n",
        "            (text_x, text_y),\n",
        "            final_text,\n",
        "            font=font,\n",
        "            fill=fg_color,\n",
        "            align='center'\n",
        "        )\n",
        "\n",
        "        np_img = np.array(img)\n",
        "        txt = ImageClip(np_img, duration=duration)\n",
        "        return txt\n",
        "    except Exception as e:\n",
        "        print(f\"[Subtitle/Pillow ultimate fail] {e}\")\n",
        "        raise RuntimeError(\"Subtitle TextClip/ImageClip creation failed with all methods.\")\n",
        "\n",
        "def create_shorts_video(image_files, narration_path, subtitles, output_path=\"shorts_final.mp4\"):\n",
        "    audio = AudioFileClip(narration_path)\n",
        "    total_duration = audio.duration\n",
        "    num_images = len(image_files)\n",
        "    img_duration = total_duration / num_images\n",
        "\n",
        "    # Prepare image clips (robust resize)\n",
        "    clips = []\n",
        "    for idx, img in enumerate(image_files):\n",
        "        base_clip = ImageClip(img)\n",
        "        base_clip = resize_fx(base_clip, (1080, 1920))\n",
        "        base_clip = _set_duration(base_clip, img_duration)\n",
        "        base_clip = _set_position(base_clip, 'center')\n",
        "        clips.append(base_clip)\n",
        "    video = concatenate_videoclips(clips, method=\"compose\")\n",
        "    video = _set_audio(video, audio)\n",
        "\n",
        "    # Prepare subtitle clips (with fade-in/out for nicer effect)\n",
        "    subtitle_clips = []\n",
        "    for sub in subtitles:\n",
        "        txt = robust_textclip(\n",
        "            sub[\"text\"],\n",
        "            sub[\"start\"],\n",
        "            sub[\"end\"],\n",
        "            TEXT_FONT_PATH,\n",
        "            TEXT_FONTSIZE\n",
        "        )\n",
        "        txt = _set_start(txt, sub[\"start\"])\n",
        "        txt = _set_duration(txt, sub[\"end\"] - sub[\"start\"])\n",
        "        txt = _set_position(txt, (\"center\", 120))\n",
        "        # Only call margin if TextClip (not ImageClip)\n",
        "        if isinstance(txt, TextClip):\n",
        "            txt = txt.margin(top=10, opacity=0)\n",
        "        subtitle_clips.append(txt)\n",
        "\n",
        "    # Overlay subtitles\n",
        "    final = CompositeVideoClip([video] + subtitle_clips)\n",
        "    final.write_videofile(output_path, fps=24, codec=\"libx264\", audio_codec=\"aac\")\n",
        "    print(f\"Final shorts video saved to {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "SHORTS_VIDEO_PATH = create_shorts_video(IMAGE_FILES, NARRATION_AUDIO_PATH, SUBTITLES)"
      ],
      "metadata": {
        "id": "r0sX0Rg7vCOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#@📝 8. Generate Metadata"
      ],
      "metadata": {
        "id": "gQxaYloyApEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Generate Metadata (Story-based, clickworthy, Shorts-optimized)\n",
        "def parse_metadata_response(text):\n",
        "    title, desc, tags = None, None, []\n",
        "    for line in text.splitlines():\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"title:\"):\n",
        "            title = line.split(\":\",1)[1].strip().strip(' \"\\'')\n",
        "            if title and not title[-1] in ('!', '?', '.'):\n",
        "                title += '!'\n",
        "        if low.startswith(\"description:\"):\n",
        "            desc = line.split(\":\",1)[1].strip()\n",
        "            if desc:\n",
        "                desc = desc\n",
        "        if low.startswith(\"tags:\"):\n",
        "            tags = [t.strip() for t in line.split(\":\",1)[1].split(\",\")]\n",
        "    required_tags = [\"#shorts\", \"#short\",\"#Shortsviral\", \"fyp\", \"viralshorts\", \"viralnews\", \"globalnews\", \"#trending\", \"#youtubeshorts\"]\n",
        "    tags = [tag for tag in tags if tag]\n",
        "    for req_tag in required_tags:\n",
        "        if req_tag.lower() not in [t.lower() for t in tags]:\n",
        "            tags.append(req_tag)\n",
        "    tags = [tag[:25] for tag in tags if tag]\n",
        "    tags = list(set(tags))[:30]\n",
        "    return title, desc, tags\n",
        "\n",
        "def generate_metadata_from_story(story_text):\n",
        "    prompt = (\n",
        "        f\"You are an expert at viral YouTube Shorts. Given the following short story, create:\\n\"\n",
        "        \"1. A clickworthy, emotional TITLE (<85 chars) based on the most powerful moment, surprise, or twist (NO bland summaries, NO boring names, must create curiosity!) and include 2-3 relevant hashtags\\n\"\n",
        "        \"2. A DESCRIPTION (<3500 characters) of the story, opening with a main keyword, includes a call to action (Like/Comment/Share), ends with a question. Then add 20-30 relevant hashtags at the end.\\n\"\n",
        "        \"3. Around 30 SEO TAGS (comma-separated), mixing story keywords, emotional triggers, setting, and all required viral tags (#shorts, #youtubeshorts, etc).\\n\"\n",
        "        \"\\nStory:\\n\"\n",
        "        f\"{story_text}\\n\"\n",
        "        \"\\nFormat:\\n\"\n",
        "        \"Title: ...\\nDescription: ...\\nTags: ...\\n\"\n",
        "    )\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.8,\n",
        "        max_tokens=3000\n",
        "    )\n",
        "    text = resp.choices[0].message.content.strip()\n",
        "    return parse_metadata_response(text)\n",
        "\n",
        "# Example usage (replace STORY_SCRIPT with your actual story variable)\n",
        "title, description, tags = generate_metadata_from_story(STORY_SCRIPT)\n",
        "print(\"📝 Metadata:\\n \", title, \"\\n \", description, \"\\n \", tags)\n"
      ],
      "metadata": {
        "id": "EC2-DOmwApZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📤 9. Upload to YouTube"
      ],
      "metadata": {
        "id": "Po9hWorbAtHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 9. Upload to YouTube ───────────────────────────────────────────────\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.errors import HttpError\n",
        "import time, os\n",
        "\n",
        "def upload_video_to_youtube(\n",
        "    file_path, title, description, tags,\n",
        "    category_id=22, privacy=\"public\",\n",
        "    chunk_size=1024*1024, max_retries=5\n",
        "):\n",
        "    # Ensure tags is a list (not a comma-separated string)\n",
        "    if isinstance(tags, str):\n",
        "        tags = [t.strip() for t in tags.split(\",\") if t.strip()]\n",
        "\n",
        "    # Enforce Shorts requirements\n",
        "    if not any(t.lower() == \"#shorts\" for t in tags):\n",
        "        tags.append(\"#shorts\")\n",
        "\n",
        "    body = {\n",
        "        \"snippet\": {\n",
        "            \"title\": title,\n",
        "            \"description\": f\"{description}\\n\\n#shorts\",\n",
        "            \"tags\": tags,\n",
        "            \"categoryId\": str(category_id)\n",
        "        },\n",
        "        \"status\": {\n",
        "            \"privacyStatus\": privacy,\n",
        "            \"selfDeclaredMadeForKids\": False\n",
        "        }\n",
        "        # YouTube ignores contentDetails.duration in upload, but safe to keep.\n",
        "    }\n",
        "\n",
        "    media = MediaFileUpload(\n",
        "        file_path,\n",
        "        mimetype='video/mp4',\n",
        "        chunksize=chunk_size,\n",
        "        resumable=True\n",
        "    )\n",
        "\n",
        "    req = youtube.videos().insert(\n",
        "        part=\"snippet,status\",\n",
        "        body=body,\n",
        "        media_body=media\n",
        "    )\n",
        "\n",
        "    done = False\n",
        "    retry = 0\n",
        "    while not done:\n",
        "        try:\n",
        "            status, resp = req.next_chunk()\n",
        "            if status:\n",
        "                print(f\"🟢 {int(status.progress()*100)}% uploaded\")\n",
        "            else:\n",
        "                done = True\n",
        "        except HttpError as e:\n",
        "            if e.resp.status in [500,502,503,504] and retry < max_retries:\n",
        "                retry += 1\n",
        "                time.sleep(2**retry)\n",
        "                print(f\"⚠️ Retry {retry}\")\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    print(\"✅ Upload complete! Video ID:\", resp['id'])\n",
        "    return resp\n",
        "\n",
        "# Usage with new metadata:\n",
        "response = upload_video_to_youtube(\"shorts_final.mp4\", title, description, tags)\n",
        "print(\"🎉 Done! https://youtu.be/\" + response[\"id\"])\n"
      ],
      "metadata": {
        "id": "6Oy8j6CEAtcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}